{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Para cada palavra na lista abaixo, identifique seu lema (forma base):\n",
    "running = run\n",
    "better = best\n",
    "studies = study\n",
    "wolves = wolf\n",
    "mice = mouse\n",
    "children = child\n",
    "was = are\n",
    "ate = eat\n",
    "swimming = swin\n",
    "parties = party\n",
    "leaves = left\n",
    "knives = knive\n",
    "happier = happy\n",
    "studying = study\n",
    "played = play\n",
    "goes = go\n",
    "driving = drive\n",
    "talked = talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Para cada frase, identifique os lemas de todas as palavras\n",
    "1. \"The children were playing in the leaves yesterday.\" = playing (play)\n",
    "2. \"She studies computer science and is taking three courses.\" =  studies (study) e taking (take)\n",
    "3. \"The wolves howled at the moon while mice scurried in the grass.\" howled (how) e scurried (scurry)\n",
    "4. \"He was driving faster than the cars around him.\" = driving (drive)\n",
    "5. \"The chefs used sharp knives to prepare the tastiest dishes = sharp (sharp) e dishes (dish)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Use a função WordNetLemmatizer() do NLTK para verificar suas respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando bibliotecas necessárias para realizar a limpeza e tratamento dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixando recursos\n",
    "###### esse bloco baixa os recursos do  NLKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Baixar recursos necessários do NLTK\n",
    "nltk.download('stopwords', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('wordnet', download_dir='/home/codespace/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='/home/codespace/nltk_data')  # Open Multilingual WordNet\n",
    "nltk.data.path.append('/home/codespace/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(text):\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontuações\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza um texto dividindo por espaços\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords da lista de tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lematiza uma lista de tokens usando WordNetLemmatizer do NLTK\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após limpeza básica: running better studies wolves mice children was ate swimming parties leaves knives happier studying played goes driving talked\n",
      "Após tokenização: ['running', 'better', 'studies', 'wolves', 'mice', 'children', 'was', 'ate', 'swimming', 'parties', 'leaves', 'knives', 'happier', 'studying', 'played', 'goes', 'driving', 'talked']\n",
      "Após remoção de stopwords: ['running', 'better', 'studies', 'wolves', 'mice', 'children', 'ate', 'swimming', 'parties', 'leaves', 'knives', 'happier', 'studying', 'played', 'goes', 'driving', 'talked']\n",
      "Após lematização: ['running', 'better', 'study', 'wolf', 'mouse', 'child', 'ate', 'swimming', 'party', 'leaf', 'knife', 'happier', 'studying', 'played', 'go', 'driving', 'talked']\n"
     ]
    }
   ],
   "source": [
    "#Tratando a primeira frase\n",
    "\n",
    "#Agrupei todas as palavras da lista em uma única frase\n",
    "primeirafrase = 'Running, better, studies, wolves, mice, children, was, ate, swimming, parties, leaves, knives, happier, studying, played, goes, driving, talked.'\n",
    "\n",
    "# Etapa 1: Limpeza básica\n",
    "cleaned_sentence = basic_cleaning(primeirafrase)\n",
    "print(\"Após limpeza básica:\", cleaned_sentence)\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(cleaned_sentence)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: A primeira etapa (limpeza básica) não será mais necessária a partir daqui, pois as palavras das próximas frases já estão separadas por espaço e a string não contém caracteres especiais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['The', 'children', 'were', 'playing', 'in', 'the', 'leaves', 'yesterday.']\n",
      "Após remoção de stopwords: ['The', 'children', 'playing', 'leaves', 'yesterday.']\n",
      "Após lematização: ['The', 'child', 'playing', 'leaf', 'yesterday.']\n"
     ]
    }
   ],
   "source": [
    "#Tratando a segunda frase\n",
    "segundafrase = \"The children were playing in the leaves yesterday.\"\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(segundafrase)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['She', 'studies', 'computer', 'science', 'and', 'is', 'taking', 'three', 'courses.']\n",
      "Após remoção de stopwords: ['She', 'studies', 'computer', 'science', 'taking', 'three', 'courses.']\n",
      "Após lematização: ['She', 'study', 'computer', 'science', 'taking', 'three', 'courses.']\n"
     ]
    }
   ],
   "source": [
    "#Tratando a terceira frase\n",
    "terceirafrase=\"She studies computer science and is taking three courses.\"\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(terceirafrase)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['The', 'wolves', 'howled', 'at', 'the', 'moon', 'while', 'mice', 'scurried', 'in', 'the', 'grass.']\n",
      "Após remoção de stopwords: ['The', 'wolves', 'howled', 'moon', 'mice', 'scurried', 'grass.']\n",
      "Após lematização: ['The', 'wolf', 'howled', 'moon', 'mouse', 'scurried', 'grass.']\n"
     ]
    }
   ],
   "source": [
    "#Tratando a quarta frase\n",
    "\n",
    "quartafrase=\"The wolves howled at the moon while mice scurried in the grass.\"\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(quartafrase)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['running', 'better', 'studies', 'wolves', 'mice', 'children', 'was', 'ate', 'swimming', 'parties', 'leaves', 'knives', 'happier', 'studying', 'played', 'goes', 'driving', 'talked']\n",
      "Após remoção de stopwords: ['running', 'better', 'studies', 'wolves', 'mice', 'children', 'ate', 'swimming', 'parties', 'leaves', 'knives', 'happier', 'studying', 'played', 'goes', 'driving', 'talked']\n",
      "Após lematização: ['running', 'better', 'study', 'wolf', 'mouse', 'child', 'ate', 'swimming', 'party', 'leaf', 'knife', 'happier', 'studying', 'played', 'go', 'driving', 'talked']\n"
     ]
    }
   ],
   "source": [
    "#Tratando a quinta frase\n",
    "quintafrase=\"He was driving faster than the cars around him.\"\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(cleaned_sentence)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após tokenização: ['The', 'chefs', 'used', 'sharp', 'knives', 'to', 'prepare', 'the', 'tastiest', 'dishes.']\n",
      "Após remoção de stopwords: ['The', 'chefs', 'used', 'sharp', 'knives', 'prepare', 'tastiest', 'dishes.']\n",
      "Após lematização: ['The', 'chef', 'used', 'sharp', 'knife', 'prepare', 'tastiest', 'dishes.']\n"
     ]
    }
   ],
   "source": [
    "#Tratando a sexta frase\n",
    "\n",
    "sextafrase=\"The chefs used sharp knives to prepare the tastiest dishes.\"\n",
    "\n",
    "# Etapa 2: Tokenização simples\n",
    "tokens = simple_tokenize(sextafrase)\n",
    "print(\"Após tokenização:\", tokens)\n",
    "\n",
    "# Etapa 3: Remoção de stopwords\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(\"Após remoção de stopwords:\", filtered_tokens)\n",
    "\n",
    "# Etapa 4: Lematização\n",
    "lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
    "print(\"Após lematização:\", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
