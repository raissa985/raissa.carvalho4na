{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.9.3)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.8.5)\n",
      "Requirement already satisfied: plotly in /home/codespace/.local/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim matplotlib scikit-learn pandas numpy spacy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações básicas\n",
    "import numpy as np #trabalhar matrizes\n",
    "import pandas as pd #trabalhar dataframes\n",
    "import matplotlib.pyplot as plt #gráficos\n",
    "from gensim.models import Word2Vec #modelo de vetorização\n",
    "import gensim.downloader as api \n",
    "from sklearn.manifold import TSNE #framework de treinamento\n",
    "import re #regex\n",
    "import nltk #trabalho de frases e trabalhos\n",
    "from nltk.tokenize import word_tokenize \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download de recursos NLTK (se necessário)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto 1: Este filme é incrível, adorei a atuação do protagonista\n",
      "Texto 2: A direção de fotografia é espetacular e o roteiro é envolvente\n",
      "Texto 3: Péssimo filme, desperdicei meu tempo assistindo isso\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo - críticas de filmes (simplificadas)\n",
    "textos = [\n",
    "    \"Este filme é incrível, adorei a atuação do protagonista\",\n",
    "    \"A direção de fotografia é espetacular e o roteiro é envolvente\",\n",
    "    \"Péssimo filme, desperdicei meu tempo assistindo isso\",\n",
    "    \"Os atores são talentosos mas o roteiro é fraco\",\n",
    "    \"Cinematografia belíssima, recomendo assistir no cinema\",\n",
    "    \"Não gostei da história, personagens mal desenvolvidos\",\n",
    "    \"A trilha sonora combina perfeitamente com as cenas\",\n",
    "    \"Filme entediante, previsível do início ao fim\",\n",
    "    \"Os efeitos especiais são impressionantes, tecnologia de ponta\",\n",
    "    \"História emocionante, chorei no final do filme\"\n",
    "]\n",
    "\n",
    "# Verificando os dados\n",
    "for i, texto in enumerate(textos[:3]):  # Mostrando apenas os 3 primeiros\n",
    "    print(f\"Texto {i+1}: {texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de texto original:\n",
      "Este filme é incrível, adorei a atuação do protagonista\n",
      "\n",
      "Depois do pré-processamento:\n",
      "['filme', 'incrível', 'adorei', 'atuação', 'protagonista']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    texto = re.sub(r'[^a-záàâãéèêíïóôõöúçñ ]', '', texto)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(texto)\n",
    "\n",
    "    # Remover stopwords (opcional, dependendo da aplicação)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento a todos os textos\n",
    "textos_preprocessados = [preprocessar_texto(texto) for texto in textos]\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de texto original:\")\n",
    "print(textos[0])\n",
    "print(\"\\nDepois do pré-processamento:\")\n",
    "print(textos_preprocessados[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parâmetros do modelo\n",
    "vector_size = 100    # Dimensionalidade dos vetores\n",
    "window = 5           # Tamanho da janela de contexto\n",
    "min_count = 1        # Frequência mínima das palavras\n",
    "workers = 4          # Número de threads para treinamento\n",
    "sg = 1               # Modelo Skip-gram (1) ou CBOW (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "model = Word2Vec(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com 44 palavras no vocabulário\n"
     ]
    }
   ],
   "source": [
    "print(f\"Modelo treinado com {len(model.wv.key_to_index)} palavras no vocabulário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas palavras do vocabulário:\n",
      "['filme', 'história', 'roteiro', 'desperdicei', 'belíssima', 'cinematografia', 'fraco', 'talentosos', 'atores', 'assistindo', 'tempo', 'péssimo', 'assistir', 'envolvente', 'espetacular', 'fotografia', 'direção', 'protagonista', 'atuação', 'adorei', 'incrível', 'recomendo', 'final', 'chorei', 'previsível', 'emocionante', 'ponta', 'tecnologia', 'impressionantes', 'especiais', 'efeitos', 'fim', 'início', 'entediante', 'gostei', 'cenas', 'perfeitamente', 'combina', 'sonora', 'trilha', 'desenvolvidos', 'mal', 'personagens', 'cinema']\n"
     ]
    }
   ],
   "source": [
    "# Listar algumas palavras do vocabulário\n",
    "palavras = list(model.wv.key_to_index.keys())\n",
    "print(\"Algumas palavras do vocabulário:\")\n",
    "print(palavras)  # Primeiras 10 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas palavras do vocabulário:\n",
      "1- filme\n",
      "2- história\n",
      "3- roteiro\n",
      "4- desperdicei\n",
      "5- belíssima\n",
      "6- cinematografia\n",
      "7- fraco\n",
      "8- talentosos\n",
      "9- atores\n",
      "10- assistindo\n",
      "11- tempo\n",
      "12- péssimo\n",
      "13- assistir\n",
      "14- envolvente\n",
      "15- espetacular\n",
      "16- fotografia\n",
      "17- direção\n",
      "18- protagonista\n",
      "19- atuação\n",
      "20- adorei\n",
      "21- incrível\n",
      "22- recomendo\n",
      "23- final\n",
      "24- chorei\n",
      "25- previsível\n",
      "26- emocionante\n",
      "27- ponta\n",
      "28- tecnologia\n",
      "29- impressionantes\n",
      "30- especiais\n",
      "31- efeitos\n",
      "32- fim\n",
      "33- início\n",
      "34- entediante\n",
      "35- gostei\n",
      "36- cenas\n",
      "37- perfeitamente\n",
      "38- combina\n",
      "39- sonora\n",
      "40- trilha\n",
      "41- desenvolvidos\n",
      "42- mal\n",
      "43- personagens\n",
      "44- cinema\n"
     ]
    }
   ],
   "source": [
    "palavras = list(model.wv.key_to_index.keys())\n",
    "\n",
    "print(\"Algumas palavras do vocabulário:\")\n",
    "for i, palavra in enumerate(palavras, start=1):\n",
    "    print(f\"{i}- {palavra}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3898495e-04  2.3754722e-04  5.1084701e-03  9.0095662e-03\n",
      " -9.3060182e-03 -7.1179080e-03  6.4590140e-03  8.9732325e-03\n",
      " -5.0089178e-03 -3.7637462e-03  7.3839580e-03 -1.5429476e-03\n",
      " -4.5339474e-03  6.5549053e-03 -4.8634596e-03 -1.8150365e-03\n",
      "  2.8793407e-03  9.9878211e-04 -8.2852831e-03 -9.4514769e-03\n",
      "  7.3103900e-03  5.0675846e-03  6.7592384e-03  7.6005177e-04\n",
      "  6.3530109e-03 -3.4019963e-03 -9.5264451e-04  5.7722717e-03\n",
      " -7.5249239e-03 -3.9323601e-03 -7.5148693e-03 -9.2767383e-04\n",
      "  9.5453048e-03 -7.3271766e-03 -2.3372965e-03 -1.9302175e-03\n",
      "  8.0870856e-03 -5.9312731e-03  4.2869444e-05 -4.7512343e-03\n",
      " -9.6031642e-03  5.0065401e-03 -8.7594036e-03 -4.3953191e-03\n",
      " -3.3294396e-05 -2.9962539e-04 -7.6586660e-03  9.6185729e-03\n",
      "  4.9843234e-03  9.2339637e-03 -8.1531862e-03  4.4953576e-03\n",
      " -4.1356767e-03  8.2382484e-04  8.4956065e-03 -4.4643418e-03\n",
      "  4.5180386e-03 -6.7829816e-03 -3.5452251e-03  9.4016502e-03\n",
      " -1.5767787e-03  3.2035878e-04 -4.1417871e-03 -7.6862508e-03\n",
      " -1.5016500e-03  2.4671410e-03 -8.8506279e-04  5.5344566e-03\n",
      " -2.7436947e-03  2.2614619e-03  5.4569058e-03  8.3484771e-03\n",
      " -1.4522823e-03 -9.2028687e-03  4.3761330e-03  5.7426962e-04\n",
      "  7.4392390e-03 -8.1702234e-04 -2.6337255e-03 -8.7473402e-03\n",
      " -8.5956510e-04  2.8262755e-03  5.4018986e-03  7.0487335e-03\n",
      " -5.7079527e-03  1.8547688e-03  6.0868002e-03 -4.8000212e-03\n",
      " -3.1051093e-03  6.8035545e-03  1.6332354e-03  1.9418275e-04\n",
      "  3.4722297e-03  2.1274926e-04  9.6273180e-03  5.0582150e-03\n",
      " -8.9172907e-03 -7.0370454e-03  8.9642097e-04  6.3917381e-03]\n"
     ]
    }
   ],
   "source": [
    "if 'filme' in model.wv:\n",
    "    vector_filme = model.wv['filme']\n",
    "    print(vector_filme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vetor da palavra 'filme' (primeiras 10 dimensões):\n",
      "[-0.00053898  0.00023755  0.00510847  0.00900957 -0.00930602 -0.00711791\n",
      "  0.00645901  0.00897323 -0.00500892 -0.00376375]\n",
      "Dimensionalidade do vetor: 100\n"
     ]
    }
   ],
   "source": [
    "# Verificar o vetor de uma palavra específica\n",
    "if 'filme' in model.wv:\n",
    "    vetor_filme = model.wv['filme']\n",
    "    print(f\"\\nVetor da palavra 'filme' (primeiras 10 dimensões):\")\n",
    "    print(vetor_filme[:10])\n",
    "    print(f\"Dimensionalidade do vetor: {len(vetor_filme)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas palavras do vocabulário:\n",
      "['filme', 'história', 'roteiro', 'desperdicei', 'belíssima', 'cinematografia', 'fraco', 'talentosos', 'atores', 'assistindo']\n",
      "\n",
      "Vetor da palavra 'filme' (primeiras 10 dimensões):\n",
      "[-0.00053898  0.00023755  0.00510847  0.00900957 -0.00930602 -0.00711791\n",
      "  0.00645901  0.00897323 -0.00500892 -0.00376375]\n",
      "Dimensionalidade do vetor: 100\n"
     ]
    }
   ],
   "source": [
    "# Listar algumas palavras do vocabulário\n",
    "palavras = list(model.wv.key_to_index.keys())\n",
    "print(\"Algumas palavras do vocabulário:\")\n",
    "print(palavras[:10])  # Primeiras 10 palavras\n",
    "\n",
    "# Verificar o vetor de uma palavra específica\n",
    "if 'filme' in model.wv:\n",
    "    vetor_filme = model.wv['filme']\n",
    "    print(f\"\\nVetor da palavra 'filme' (primeiras 10 dimensões):\")\n",
    "    print(vetor_filme[:10])\n",
    "    print(f\"Dimensionalidade do vetor: {len(vetor_filme)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras mais similares a 'filme':\n",
      "direção: 0.2189\n",
      "assistindo: 0.2168\n",
      "impressionantes: 0.1955\n",
      "cinema: 0.1694\n",
      "efeitos: 0.1520\n"
     ]
    }
   ],
   "source": [
    "# Encontrar palavras mais similares a 'filme'\n",
    "if 'filme' in model.wv:\n",
    "    similares = model.wv.most_similar('filme', topn=5)\n",
    "    print(\"\\nPalavras mais similares a 'filme':\")\n",
    "    for palavra, similaridade in similares:\n",
    "        print(f\"{palavra}: {similaridade:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Salvar o modelo\n",
    "model.save(\"word2vec_filmes.model\")\n",
    "\n",
    "# Carregando o modelo salvo\n",
    "modelo_carregado = Word2Vec.load(\"word2vec_filmes.model\")\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alguns modelos disponíveis no gensim-data:\n",
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50']\n",
      "Modelo pré-treinado carregado com 400000 palavras\n",
      "\n",
      "Palavras mais similares a 'computer':\n",
      "computers: 0.8752\n",
      "software: 0.8373\n",
      "technology: 0.7642\n",
      "pc: 0.7366\n",
      "hardware: 0.7290\n"
     ]
    }
   ],
   "source": [
    "# Listar modelos disponíveis\n",
    "print(\"Alguns modelos disponíveis no gensim-data:\")\n",
    "print([nome for nome in list(api.info()['models'].keys())[:10]])\n",
    "\n",
    "# Carregar um modelo pré-treinado (word2vec-google-news-300)\n",
    "# Nota: Isso pode demorar um pouco na primeira execução (download)\n",
    "try:\n",
    "    # Para português, você pode tentar 'word2vec-portuguese' se disponível\n",
    "    # ou usar modelos do NILC: http://nilc.icmc.usp.br/embeddings\n",
    "    modelo_pretreino = api.load(\"glove-wiki-gigaword-100\")  # Mais rápido que word2vec-google-news-300\n",
    "    print(f\"Modelo pré-treinado carregado com {len(modelo_pretreino.key_to_index)} palavras\")\n",
    "\n",
    "    # Verificar palavras similares usando modelo pré-treinado\n",
    "    if 'computer' in modelo_pretreino:\n",
    "        similares = modelo_pretreino.most_similar('computer', topn=5)\n",
    "        print(\"\\nPalavras mais similares a 'computer':\")\n",
    "        for palavra, similaridade in similares:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar modelo pré-treinado: {e}\")\n",
    "    print(\"Tente outro modelo ou prossiga sem esta parte\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogia: rei - homem + mulher =\n",
      "queen: 0.7699\n",
      "monarch: 0.6843\n",
      "throne: 0.6756\n"
     ]
    }
   ],
   "source": [
    "# Usando modelo pré-treinado para analogias (se disponível)\n",
    "try:\n",
    "    # Famosa analogia: rei - homem + mulher = rainha\n",
    "    if all(word in modelo_pretreino for word in ['king', 'man', 'woman']):\n",
    "        resultado = modelo_pretreino.most_similar(\n",
    "            positive=['king', 'woman'],\n",
    "            negative=['man'],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nAnalogia: rei - homem + mulher =\")\n",
    "        for palavra, similaridade in resultado:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except:\n",
    "    print(\"Não foi possível realizar a operação de analogia com o modelo disponível\")\n",
    "\n",
    "# Podemos tentar outras analogias com nosso modelo treinado\n",
    "# Por exemplo: bom - positivo + negativo = ruim\n",
    "# (Dependendo do tamanho do corpus, pode não funcionar bem para modelos pequenos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similaridade entre pares de palavras:\n",
      "filme - cinema: 0.1693534404039383\n",
      "bom - ruim: Uma ou ambas as palavras não estão no vocabulário\n",
      "ator - atuação: Uma ou ambas as palavras não estão no vocabulário\n",
      "filme - protagonista: -0.11359719187021255\n"
     ]
    }
   ],
   "source": [
    "# Função para calcular similaridade entre pares de palavras\n",
    "def calcular_similaridade(modelo, pares_palavras):\n",
    "    resultados = []\n",
    "    for par in pares_palavras:\n",
    "        palavra1, palavra2 = par\n",
    "        if palavra1 in modelo.wv and palavra2 in modelo.wv:\n",
    "            similaridade = modelo.wv.similarity(palavra1, palavra2)\n",
    "            resultados.append((par, similaridade))\n",
    "        else:\n",
    "            resultados.append((par, \"Uma ou ambas as palavras não estão no vocabulário\"))\n",
    "    return resultados\n",
    "\n",
    "# Pares de palavras para testar\n",
    "pares = [\n",
    "    ('filme', 'cinema'),\n",
    "    ('bom', 'ruim'),\n",
    "    ('ator', 'atuação'),\n",
    "    ('filme', 'protagonista')\n",
    "]\n",
    "\n",
    "# Calcular similaridades\n",
    "similaridades = calcular_similaridade(model, pares)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nSimilaridade entre pares de palavras:\")\n",
    "for (palavra1, palavra2), similaridade in similaridades:\n",
    "    if isinstance(similaridade, float):\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade:.4f}\")\n",
    "    else:\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_embeddings(modelo, palavras=None, n_palavras=50):\n",
    "    \"\"\"\n",
    "    Visualiza os embeddings em um espaço 2D usando t-SNE.\n",
    "    Args:\n",
    "        modelo: Modelo Word2Vec\n",
    "        palavras: Lista de palavras específicas para visualizar (opcional)\n",
    "        n_palavras: Número de palavras mais frequentes a visualizar (se palavras=None)\n",
    "    \"\"\"\n",
    "    # Obter palavras e vetores\n",
    "    if palavras is None:\n",
    "        palavras = list(modelo.wv.key_to_index.keys())[:n_palavras]\n",
    "    else:\n",
    "        # Filtrar palavras que não estão no vocabulário\n",
    "        palavras = [p for p in palavras if p in modelo.wv]\n",
    "    \n",
    "    if not palavras:\n",
    "        print(\"Nenhuma palavra válida para visualização\")\n",
    "        return\n",
    "    \n",
    "    # Obter vetores e converter para array NumPy\n",
    "    vetores = np.array([modelo.wv[palavra] for palavra in palavras])\n",
    "    \n",
    "    # Reduzir dimensionalidade com t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(palavras)-1))\n",
    "    vetores_2d = tsne.fit_transform(vetores)\n",
    "    \n",
    "    # Criar gráfico\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plotar pontos\n",
    "    x = vetores_2d[:, 0]\n",
    "    y = vetores_2d[:, 1]\n",
    "    plt.scatter(x, y, c='blue', alpha=0.7)\n",
    "    \n",
    "    # Adicionar rótulos (palavras)\n",
    "    for i, palavra in enumerate(palavras):\n",
    "        plt.annotate(\n",
    "            palavra,\n",
    "            xy=(x[i], y[i]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    plt.title(\"Visualização t-SNE dos Word Embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados com TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "\n",
      "Resultados com Word Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dados rotulados para exemplo\n",
    "textos_rotulados = textos  # Usando os mesmos textos de antes\n",
    "sentimentos = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1: positivo, 0: negativo\n",
    "\n",
    "# Função para gerar vetores de documento usando embeddings\n",
    "def texto_para_vetor(texto, modelo):\n",
    "    \"\"\"Converte um texto em um vetor médio dos embeddings de suas palavras\"\"\"\n",
    "    palavras = preprocessar_texto(texto)\n",
    "    # Filtrar palavras que estão no vocabulário do modelo\n",
    "    palavras_no_vocab = [p for p in palavras if p in modelo.wv]\n",
    "    if not palavras_no_vocab:\n",
    "        # Se nenhuma palavra estiver no vocabulário, retorna vetor de zeros\n",
    "        return np.zeros(modelo.vector_size)\n",
    "    # Calcular a média dos vetores das palavras\n",
    "    vetores = [modelo.wv[palavra] for palavra in palavras_no_vocab]\n",
    "    return np.mean(vetores, axis=0)\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    textos_rotulados, sentimentos, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Abordagem com TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "clf_tfidf = LogisticRegression(random_state=42)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# 2. Abordagem com Word Embeddings\n",
    "X_train_emb = np.array([texto_para_vetor(texto, model) for texto in X_train])\n",
    "X_test_emb = np.array([texto_para_vetor(texto, model) for texto in X_test])\n",
    "\n",
    "clf_emb = LogisticRegression(random_state=42)\n",
    "clf_emb.fit(X_train_emb, y_train)\n",
    "y_pred_emb = clf_emb.predict(X_test_emb)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\nResultados com TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "print(\"\\nResultados com Word Embeddings:\")\n",
    "print(classification_report(y_test, y_pred_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexto: O banco está cheio de\n",
      "Vetor (10 primeiras dimensões): [-2.761275    2.7341504   1.9384544   0.34338498  1.0899546  -4.300789\n",
      "  1.5131965   3.7354984   1.1549678  -1.9115787 ]\n",
      "Dimensão do vetor: 96\n",
      "--------------------------------------------------\n",
      "Contexto: Eu sentei no banco da praça.\n",
      "Vetor (10 primeiras dimensões): [-2.8606548   3.2033174   4.2152634   2.3896182   4.4038973  -3.509808\n",
      "  4.1706643   5.813535   -0.12308097 -0.5815916 ]\n",
      "Dimensão do vetor: 96\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar modelo spaCy (pequeno)\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")  # Para português\n",
    "    # Alternativa para inglês: nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Exemplo de texto\n",
    "    texto = \"O banco está cheio de dinheiro. Eu sentei no banco da praça.\"\n",
    "\n",
    "    # Processar o texto\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Examinar embeddings para cada ocorrência da palavra \"banco\"\n",
    "    for token in doc:\n",
    "        if token.text.lower() == \"banco\":\n",
    "            contexto = doc[max(0, token.i-3):min(len(doc), token.i+4)]\n",
    "            print(f\"Contexto: {contexto}\")\n",
    "            print(f\"Vetor (10 primeiras dimensões): {token.vector[:10]}\")\n",
    "            print(f\"Dimensão do vetor: {len(token.vector)}\")\n",
    "            print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar spaCy: {e}\")\n",
    "    print(\"Talvez seja necessário instalar os modelos com:\")\n",
    "    print(\"python -m spacy download pt_core_news_sm\")\n",
    "    print(\"ou\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings para palavras fora do vocabulário (FastText):\n",
      "'filmagem' (no vocabulário: False)\n",
      "Primeiras 5 dimensões do vetor: [-0.00071874  0.00085034  0.00017239 -0.0009094  -0.00061973]\n",
      "Palavras similares a 'filmagem':\n",
      "  gostei: 0.2725\n",
      "  filme: 0.2633\n",
      "  cenas: 0.2351\n",
      "\n",
      "'cinematográfico' (no vocabulário: False)\n",
      "Primeiras 5 dimensões do vetor: [ 0.00013222  0.00121597  0.00156423 -0.00238611  0.00105269]\n",
      "Palavras similares a 'cinematográfico':\n",
      "  cinematografia: 0.6115\n",
      "  cinema: 0.4938\n",
      "  trilha: 0.2005\n",
      "\n",
      "'atuações' (no vocabulário: False)\n",
      "Primeiras 5 dimensões do vetor: [ 0.00126086 -0.00013844  0.00066779  0.00134189 -0.00153145]\n",
      "Palavras similares a 'atuações':\n",
      "  atuação: 0.3585\n",
      "  personagens: 0.2834\n",
      "  roteiro: 0.1743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Treinar modelo FastText\n",
    "modelo_fasttext = FastText(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "# Testar com palavras que não estão no corpus\n",
    "palavras_teste = [\"filmagem\", \"cinematográfico\", \"atuações\"]\n",
    "\n",
    "print(\"\\nEmbeddings para palavras fora do vocabulário (FastText):\")\n",
    "for palavra in palavras_teste:\n",
    "    # Verificar se a palavra está no vocabulário original\n",
    "    no_vocab = palavra in model.wv\n",
    "    # Obter vetor do FastText (funciona mesmo para palavras fora do vocabulário)\n",
    "    vetor = modelo_fasttext.wv[palavra]\n",
    "    print(f\"'{palavra}' (no vocabulário: {no_vocab})\")\n",
    "    print(f\"Primeiras 5 dimensões do vetor: {vetor[:5]}\")\n",
    "\n",
    "    # Encontrar palavras similares\n",
    "    similares = modelo_fasttext.wv.most_similar(palavra, topn=3)\n",
    "    print(f\"Palavras similares a '{palavra}':\")\n",
    "    for p, sim in similares:\n",
    "        print(f\"  {p}: {sim:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercícios Práticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similaridade_documentos(doc1, doc2, model):\n",
    "    \"\"\"Calcula a similaridade entre dois documentos usando embeddings\"\"\"\n",
    "    vetor1 = texto_para_vetor(doc1, model)\n",
    "    vetor2 = texto_para_vetor(doc2, model)\n",
    "\n",
    "    # Calcular similaridade do cosseno\n",
    "    # similaridade = 1 - distância do cosseno\n",
    "    similaridade = np.dot(vetor1, vetor2) / (np.linalg.norm(vetor1) * np.linalg.norm(vetor2))\n",
    "    return similaridade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercício: Calcule a similaridade entre os documentos abaixo\n",
    "documento1 = \"O filme tem uma história envolvente e atuações convincentes\"\n",
    "documento2 = \"A narrativa do filme é cativante e os atores são excelentes\"\n",
    "documento3 = \"O restaurante tem comida deliciosa e preços acessíveis\"\n",
    "\n",
    "# Calcular similaridades (implemente sua solução)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
